!pip install mediapipe    # Mediapipe ì„¤ì¹˜

import mediapipe as mp                                  # ì† ëœë“œë§ˆí¬ ì˜¤ë²„ë ˆì´ ëª©ì 
import os
import pathlib
import time
from tqdm import tqdm                                   # ì§„í–‰ë¥  í‘œì‹œ
import cv2                                              # OpenCV: ì´ë¯¸ì§€ ì²˜ë¦¬
import torch
import torchvision as tv                                # PyTorch ë¹„ì „ ê´€ë ¨ ê¸°ëŠ¥
from torch import nn
from torchvision import transforms as T                 # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
from torch.utils.data import DataLoader, random_split
from torch.amp import autocast, GradScaler              # Mixed Precision í•™ìŠµ
import numpy as np
from PIL import Image                                   # ì´ë¯¸ì§€ ì…ì¶œë ¥
from google.colab import drive                          # Google Drive ë§ˆìš´íŠ¸

drive.mount('/content/drive')
!tar -C /content/drive/MyDrive -cf - asl_alphabet_train | tar -C /tmp -xvf -

SRC_ROOT = '/tmp/asl_alphabet_train'
AUG_ROOT = '/tmp/asl_alphabet_augmented'
DST_ROOT = '/tmp/asl_alphabet_augmented_overlayed'
NUM_AUG = 3      # ì›ë³¸ ì´ë¯¸ì§€ ë‹¹ ìƒì„±í•  ì¦ê°•ë³¸ ê°œìˆ˜

aug_transform = T.Compose([
    T.RandomHorizontalFlip(),
    T.RandomRotation(15),
    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
])

for d in (AUG_ROOT, DST_ROOT):
    for cls in os.listdir(SRC_ROOT):
        os.makedirs(os.path.join(d, cls), exist_ok=True)

hands = mp.solutions.hands.Hands(
    max_num_hands=1,
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5
)
drawing = mp.solutions.drawing_utils    # ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸° ìœ í‹¸ë¦¬í‹°

for cls in os.listdir(SRC_ROOT):
    src_cls = os.path.join(SRC_ROOT, cls)
    aug_cls = os.path.join(AUG_ROOT, cls)
    dst_cls = os.path.join(DST_ROOT, cls)

    for fn in tqdm(os.listdir(src_cls), desc=f"Aug+Overlay {cls}"):
        src_path = os.path.join(src_cls, fn)
        img_pil = Image.open(src_path).convert('RGB')     # PIL ì´ë¯¸ì§€ë¡œ ì½ê¸°

        # ì›ë³¸ í¬í•¨, NUM_AUGë§Œí¼ ì¦ê°•ë³¸ ìƒì„±
        variants = [img_pil] + [aug_transform(img_pil) for _ in range(NUM_AUG)]

        for i, img_aug in enumerate(variants):
            # PIL â†’ OpenCV(BGR) ë³€í™˜
            cv_img = cv2.cvtColor(np.array(img_aug), cv2.COLOR_RGB2BGR)
            # Mediapipeë¡œ ì† ëœë“œë§ˆí¬ ê²€ì¶œ
            res = hands.process(cv2.cvtColor(cv_img, cv2.COLOR_BGR2RGB))
            # ê²€ì¶œëœ ê²½ìš°, ëœë“œë§ˆí¬ ë° ì—°ê²°ì„  ì˜¤ë²„ë ˆì´
            if res.multi_hand_landmarks:
                drawing.draw_landmarks(
                    cv_img,
                    res.multi_hand_landmarks[0],
                    mp.solutions.hands.HAND_CONNECTIONS
                )
            # ê²°ê³¼ ì €ì¥: ì›ë³¸(_aug0), ì¦ê°•ë³¸(_aug1~NUM_AUG)
            out_name = fn.replace('.jpg', f'_aug{i}.jpg')
            cv2.imwrite(os.path.join(dst_cls, out_name), cv_img)

# ë¦¬ì†ŒìŠ¤ í•´ì œ
hands.close()

DATA_ROOT = pathlib.Path(DST_ROOT)
mean, std = [0.5]*3, [0.5]*3        # Normalize íŒŒë¼ë¯¸í„°

full_ds = tv.datasets.ImageFolder(DATA_ROOT, transform=None)

n_total = len(full_ds)
n_train = int(0.8 * n_total)
n_val   = n_total - n_train
train_ds, val_ds = random_split(
    full_ds,
    [n_train, n_val],
    generator=torch.Generator().manual_seed(42)
)

train_ds.dataset.transform = T.Compose([
    T.Resize((224,224)),                    # ì…ë ¥ í¬ê¸° ë§ì¶¤
    T.ToTensor(),
    T.Normalize(mean, std),
])
val_ds.dataset.transform = T.Compose([
    T.Resize((224,224)),
    T.ToTensor(),
    T.Normalize(mean, std),
])

train_loader = DataLoader(
    train_ds, batch_size=128, shuffle=True,
    num_workers=8, pin_memory=True, drop_last=True
)
val_loader = DataLoader(
    val_ds, batch_size=128, shuffle=False,
    num_workers=8, pin_memory=True
)

print(f"â–¶ train samples: {len(train_ds)}, val samples: {len(val_ds)}")

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

model = tv.models.mobilenet_v2(weights="IMAGENET1K_V1")
model.classifier[1] = nn.Linear(model.last_channel, len(full_ds.classes))
model = model.to(device)

if device.type == 'cuda':
    model = torch.compile(model)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)
scaler    = GradScaler()
EPOCHS    = 10

for epoch in range(1, EPOCHS+1):
    # --- Training ---
    model.train()
    running_loss = 0.0
    t0 = time.time()
    for images, labels in tqdm(train_loader, desc=f"Epoch {epoch}/{EPOCHS} [Train]"):
        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)
        optimizer.zero_grad()
        with autocast(device_type='cuda'):
            outputs = model(images)
            loss    = criterion(outputs, labels)
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        running_loss += loss.item() * images.size(0)
    avg_loss = running_loss / len(train_ds)

    # --- Validation ---
    model.eval()
    val_loss = 0.0
    correct  = 0
    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            val_loss += criterion(outputs, labels).item() * images.size(0)
            preds = outputs.argmax(dim=1)
            correct += (preds == labels).sum().item()
    avg_val_loss = val_loss / len(val_ds)
    val_acc = correct / len(val_ds)

    t1 = time.time()
    print(f"Epoch {epoch} â–¶ "
          f"Train Loss: {avg_loss:.4f} | "
          f"Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f} | "
          f"Time: {t1-t0:.1f}s")

SAVE_PATH = '/content/drive/MyDrive/asl_model_local.pth'
torch.save(model.state_dict(), SAVE_PATH)
print(f"ğŸ“ Saved model to {SAVE_PATH}")